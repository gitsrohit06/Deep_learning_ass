{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#### 1.Explain the concept of forward propagation in a neural network",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nForward propagation is the process through which input data passes through the neural network layer by layer to produce an output. In this process, the network computes the weighted sum of inputs at each layer and applies an activation function to generate outputs for the next layer.\n\nThe steps involved in forward propagation are:\n\nInput Layer: The input data is fed into the neural network.\nWeight Multiplication: The input is multiplied by the weights associated with each connection between neurons.\nBias Addition: A bias term is added to the weighted input.\nActivation Function: The result of the weighted input plus bias is passed through an activation function, which introduces non-linearity to the model.\nRepeat for Each Layer: These steps are repeated for every hidden layer.\nOutput Layer: In the final layer, the network computes the output using the final set of weights and activation functions.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 2.What is the purpose of the activation function in forward propagation\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nThe activation function introduces non-linearity to the model, which is crucial for the neural network to learn and model complex patterns. Without activation functions, the network would simply be a linear transformation of the input, which limits its capacity to solve non-linear problems.\n\nLinear Activation: No non-linearity, meaning the output is simply a weighted sum of inputs.\nNon-Linear Activation: Functions like ReLU, Sigmoid, or Tanh add non-linearity, allowing the model to approximate complex functions and capture intricate patterns in the data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 3.Describe the steps involved in the backward propagation (backpropagation) algorithm'",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nBackpropagation is the process of updating the weights of a neural network based on the error of the output. It computes the gradient of the loss function with respect to each weight by applying the chain rule of calculus.\n\nThe key steps in backpropagation are:\n\nCompute the Loss: Calculate the loss (or error) between the predicted output and the actual target using a loss function (e.g., Mean Squared Error, Cross-Entropy).\n\nCalculate the Gradient of the Loss: For each layer, calculate the gradient of the loss function with respect to the weights and biases. This involves:\n\nOutput Layer: Compute the gradient of the loss with respect to the output of the layer and then backpropagate through the activation function.\nHidden Layers: For each hidden layer, backpropagate the gradient to the previous layer by calculating the partial derivative of the loss with respect to the layerâ€™s weights.\nWeight Update: Use the computed gradients to update the weights by adjusting them in the direction that minimizes the loss. Typically, a learning rate is used to control the size of the weight update.\n\nRepeat: Repeat this process for each batch of data until the loss converges to a low value.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 4.What is the purpose of the chain rule in backpropagation",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ans :\n\nThe chain rule is essential in backpropagation because it allows us to compute the gradient of the loss function with respect to each weight in the network. Since the output of each neuron depends on the weights of the previous layer, we use the chain rule to break down the derivative of the loss function into simpler parts. This enables us to efficiently propagate the error from the output layer back to the input layer, adjusting the weights at each step.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### 5. Implement the forward propagation process for a simple neural network with one hidden layer usingNumPy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Define the activation function (Sigmoid in this case)\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Define the input, weights, and biases for a simple network\n# Assume input layer has 3 neurons, hidden layer has 4 neurons, and output layer has 1 neuron\n\n# Input (batch size = 2, features = 3)\nX = np.array([[0.1, 0.2, 0.3], \n              [0.4, 0.5, 0.6]])\n\n# Weights for the hidden layer (3 input neurons, 4 hidden neurons)\nW_hidden = np.random.randn(3, 4)\n\n# Biases for the hidden layer (4 neurons in the hidden layer)\nb_hidden = np.random.randn(4)\n\n# Weights for the output layer (4 hidden neurons, 1 output neuron)\nW_output = np.random.randn(4, 1)\n\n# Bias for the output layer (1 output neuron)\nb_output = np.random.randn(1)\n\n# Forward Propagation\n# Step 1: Calculate the weighted sum for the hidden layer\nZ_hidden = np.dot(X, W_hidden) + b_hidden\n\n# Step 2: Apply activation function to the hidden layer (using Sigmoid)\nA_hidden = sigmoid(Z_hidden)\n\n# Step 3: Calculate the weighted sum for the output layer\nZ_output = np.dot(A_hidden, W_output) + b_output\n\n# Step 4: Apply activation function to the output (Sigmoid for a binary classification task)\nA_output = sigmoid(Z_output)\n\n# Print the output\nprint(\"Output of the network:\")\nprint(A_output)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Output of the network:\n[[0.43832316]\n [0.4846041 ]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    }
  ]
}